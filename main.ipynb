{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import openpyxl\n",
    "\n",
    "from data_preprocessing import encode_categorical_features\n",
    "from sampling import create_stratified_kfolds\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, RFE, SelectFromModel, f_classif, mutual_info_classif, chi2, RFE, RFA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, \\\n",
    "    matthews_corrcoef, mean_squared_error, r2_score, roc_auc_score, roc_curve, auc\n",
    "from math import sqrt\n",
    "\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook parameters\n",
    "input_dataset_path = \"data/heart_disease_health_indicators_BRFSS2015.csv\"\n",
    "target_col = \"HeartDiseaseorAttack\"\n",
    "generate_new_folds = False\n",
    "n_splits = 5\n",
    "k_best_features = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_pred: np.array, y_test: pd.Series):\n",
    "    \"\"\" Calculate model quality metrics based on \n",
    "        expected label values from testing dataset (y_test) and predicted values.\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = calculate_test_results_from_confusion_matrix(y_test, y_pred)\n",
    "    model_precision = precision_score(y_test, y_pred)\n",
    "    model_recall = recall_score(y_test, y_pred) # sensitivity\n",
    "    model_specificity = specificity_score(tn, fp)\n",
    "    model_acc = accuracy_score(y_test, y_pred)\n",
    "    model_npv = calculate_npv(tn, fn)\n",
    "\n",
    "    model_f1_score = f1_score(y_test, y_pred)\n",
    "    model_mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "    model_r2 = r2_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "    model_scores = {\n",
    "        \"True Negative\": tn,\n",
    "        \"False Positive\": fp,\n",
    "        \"False Negative\": fn,\n",
    "        \"True Positive\": tp,\n",
    "        \"Precision (PPV)\": model_precision,\n",
    "        \"Sensitivity (TPR, Recall)\": model_recall,\n",
    "        \"Speciticity (TNR)\": model_specificity,\n",
    "        \"Accuracy\": model_acc,\n",
    "        \"Negative Predictive Value (NPV)\": model_npv,\n",
    "        \"F1 Score\": model_f1_score,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R Squared\": model_r2,\n",
    "        \"Matthews Correlation Coefficient (MCC)\": model_mcc,\n",
    "        \"Threshold (from ROC Curve)\": list(thresholds),\n",
    "        \"False Positive Rate (FPR)\": list(fpr),\n",
    "        \"ROC AUC score\": roc_auc\n",
    "    }\n",
    "\n",
    "    return model_scores\n",
    "\n",
    "def calculate_test_results_from_confusion_matrix(y_test: pd.DataFrame, y_pred: pd.DataFrame):\n",
    "    \"\"\" Calculate the confusion matrix and extract TP, FP, TN, FN from that matrix \"\"\"\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "    return tn, fp, fn, tp\n",
    "\n",
    "def specificity_score(tn: float, fp: float):\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "def calculate_npv(tn: float, fn: float):\n",
    "    return tn / (tn + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_xml(dictionary, root_name='root'):\n",
    "    root = ET.Element(root_name)\n",
    "    for key, value in dictionary.items():\n",
    "        if isinstance(value, dict):\n",
    "            root.append(dict_to_xml(value, key))\n",
    "        else:\n",
    "            element = ET.Element(key)\n",
    "            element.text = str(value)\n",
    "            root.append(element)\n",
    "    return root\n",
    "\n",
    "\n",
    "def save_dict_to_xml(dictionary, file_path, root_name='root'):\n",
    "    root = dict_to_xml(dictionary, root_name)\n",
    "    tree = ET.ElementTree(root)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        tree.write(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>...</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HeartDiseaseorAttack  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
       "0                     0     1.0       1.0        1.0  40.0     1.0     0.0   \n",
       "1                     0     0.0       0.0        0.0  25.0     1.0     0.0   \n",
       "2                     0     1.0       1.0        1.0  28.0     0.0     0.0   \n",
       "3                     0     1.0       0.0        1.0  27.0     0.0     0.0   \n",
       "4                     0     1.0       1.0        1.0  24.0     0.0     0.0   \n",
       "\n",
       "   Diabetes  PhysActivity  Fruits  ...  AnyHealthcare  NoDocbcCost  GenHlth  \\\n",
       "0       0.0           0.0     0.0  ...            1.0          0.0      5.0   \n",
       "1       0.0           1.0     0.0  ...            0.0          1.0      3.0   \n",
       "2       0.0           0.0     1.0  ...            1.0          1.0      5.0   \n",
       "3       0.0           1.0     1.0  ...            1.0          0.0      2.0   \n",
       "4       0.0           1.0     1.0  ...            1.0          0.0      2.0   \n",
       "\n",
       "   MentHlth  PhysHlth  DiffWalk  Sex   Age  Education  Income  \n",
       "0      18.0      15.0       1.0  0.0   9.0        4.0     3.0  \n",
       "1       0.0       0.0       0.0  0.0   7.0        6.0     1.0  \n",
       "2      30.0      30.0       1.0  0.0   9.0        4.0     8.0  \n",
       "3       0.0       0.0       0.0  0.0  11.0        3.0     6.0  \n",
       "4       3.0       0.0       0.0  0.0  11.0        5.0     4.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_df = pd.read_csv(input_dataset_path)\n",
    "heart_df[target_col] = heart_df[target_col].astype(int)\n",
    "heart_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset has 253680 rows and 22 colums\n",
      "Input dataset consists of 21 features and 1 target column\n",
      "Target values are: [0 1]\n",
      "Input dataset contains 23899 duplicated rows and 229781 unique rows\n"
     ]
    }
   ],
   "source": [
    "# general dataset descriptors\n",
    "print(f\"Input dataset has {heart_df.shape[0]} rows and {heart_df.shape[1]} colums\")\n",
    "print(f\"Input dataset consists of {heart_df.drop(columns=[target_col]).shape[1]} features and 1 target column\")\n",
    "\n",
    "print(f\"Target values are: {heart_df[target_col].unique()}\")\n",
    "print(f\"Input dataset contains {heart_df[heart_df.duplicated()].shape[0]} duplicated rows and {heart_df[heart_df.duplicated()==False].shape[0]} unique rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 253680 entries, 0 to 253679\n",
      "Data columns (total 22 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   HeartDiseaseorAttack  253680 non-null  int32  \n",
      " 1   HighBP                253680 non-null  float64\n",
      " 2   HighChol              253680 non-null  float64\n",
      " 3   CholCheck             253680 non-null  float64\n",
      " 4   BMI                   253680 non-null  float64\n",
      " 5   Smoker                253680 non-null  float64\n",
      " 6   Stroke                253680 non-null  float64\n",
      " 7   Diabetes              253680 non-null  float64\n",
      " 8   PhysActivity          253680 non-null  float64\n",
      " 9   Fruits                253680 non-null  float64\n",
      " 10  Veggies               253680 non-null  float64\n",
      " 11  HvyAlcoholConsump     253680 non-null  float64\n",
      " 12  AnyHealthcare         253680 non-null  float64\n",
      " 13  NoDocbcCost           253680 non-null  float64\n",
      " 14  GenHlth               253680 non-null  float64\n",
      " 15  MentHlth              253680 non-null  float64\n",
      " 16  PhysHlth              253680 non-null  float64\n",
      " 17  DiffWalk              253680 non-null  float64\n",
      " 18  Sex                   253680 non-null  float64\n",
      " 19  Age                   253680 non-null  float64\n",
      " 20  Education             253680 non-null  float64\n",
      " 21  Income                253680 non-null  float64\n",
      "dtypes: float64(21), int32(1)\n",
      "memory usage: 41.6 MB\n"
     ]
    }
   ],
   "source": [
    "heart_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all duplicated values within the dataset\n",
    "heart_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordinal Categorical Features: []\n",
      "Nominal Categorical Features: []\n"
     ]
    }
   ],
   "source": [
    "# encode categorical features using LabelEncoder and OneHotEncoding\n",
    "heart_df = encode_categorical_features(heart_df, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide a heart failure dataset into features and target value sets\n",
    "x = heart_df.drop(columns=[target_col])\n",
    "y = heart_df[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folds data were loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# if needed generare and save into CSV files new folds created by stratified data sampling\n",
    "if generate_new_folds:\n",
    "    create_stratified_kfolds(x_df=x, y_df=y, dataset=heart_df, n_splits=n_splits)\n",
    "\n",
    "# Read folds that are available\n",
    "# Create empty lists to store train and test DataFrames\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "for fold_num in range(1, n_splits+1):\n",
    "    train_file_path = f\"folds/fold_{fold_num}_train.csv\"\n",
    "    test_file_path = f\"folds/fold_{fold_num}_test.csv\"\n",
    "    \n",
    "    # Load the train and test fold data into DataFrames\n",
    "    train_fold = pd.read_csv(train_file_path)\n",
    "    test_fold = pd.read_csv(test_file_path)\n",
    "    \n",
    "    train_datasets.append(train_fold)\n",
    "    test_datasets.append(test_fold)\n",
    "print(\"Folds data were loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=11, criterion=\"entropy\"),\n",
    "    \"Pruning Decision Tree\": DecisionTreeClassifier(random_state=11, ccp_alpha=0.02, criterion=\"entropy\"),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=0, C=10, penalty=\"l2\"),\n",
    "    \"Support Vector Machine\": SVC(kernel=\"linear\", C=0.3),\n",
    "    \"K Nearest Neighbours\": KNeighborsClassifier(leaf_size=1, n_neighbors=3),\n",
    "    \"Gaussina Naive Bayes\": GaussianNB(),\n",
    "    \"XGBoost Classifier\": XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective=\"binary:logistic\")\n",
    "    # ,\"CatBoost Classifier\": CatBoostClassifier(verbose=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for the folds if it doesn't exist\n",
    "if not os.path.exists('model_results'):\n",
    "    os.makedirs('model_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "folds_results = {}\n",
    "\n",
    "for fold_num, (train_data, test_data) in enumerate(zip(train_datasets, test_datasets),1):\n",
    "    # Standardize features using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(train_data.drop(target_col, axis=1))\n",
    "    X_test_scaled = scaler.transform(test_data.drop(target_col, axis=1))\n",
    "\n",
    "    # Feature Selection using SelectKBest with ANOVA F-statistic\n",
    "    selector = SelectKBest(score_func=f_classif, k=k_best_features)\n",
    "    X_train_selected = selector.fit_transform(X_train_scaled, train_data[target_col])\n",
    "    X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "    models_results_dict = {}\n",
    "    # Model Building\n",
    "    for model_name, model in models.items():\n",
    "        # train model\n",
    "        model.fit(X_train_selected, train_data[target_col])\n",
    "\n",
    "        # Model Evaluation (e.g., accuracy score)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        model_scores = calculate_metrics(y_test=test_data[target_col], y_pred=y_pred)\n",
    "        models_results_dict[model_name] = model_scores\n",
    "\n",
    "    # Add the selected feature names or indices to the dictionary\n",
    "    models_results_dict[\"selected features\"] = list(train_data.columns[selector.get_support()])\n",
    "\n",
    "    folds_results[str(fold_num)] = models_results_dict\n",
    "\n",
    "fclassif_results_df = pd.DataFrame.from_dict(folds_results) \n",
    "fclassif_results_df.to_csv(\"model_results/f_classif_feature_selection.csv\", index=False, header=True)\n",
    "\n",
    "fclassif_results_df = pd.DataFrame(folds_results)\n",
    "fclassif_results_df.to_excel(\"model_results/f_classif_feature_selection.xlsx\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "folds_results = {}\n",
    "\n",
    "for fold_num, (train_data, test_data) in enumerate(zip(train_datasets, test_datasets),1):\n",
    "    # Standardize features using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(train_data.drop(target_col, axis=1))\n",
    "    X_test_scaled = scaler.transform(test_data.drop(target_col, axis=1))\n",
    "\n",
    "    # Feature Selection Mutual Information (MI): This score function measures the dependence between two random variables. \n",
    "    # It is a good choice when you want to capture both linear and non-linear relationships between features and the target variable.\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=k_best_features)\n",
    "    X_train_selected = selector.fit_transform(X_train_scaled, train_data[target_col])\n",
    "    X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "    models_results_dict = {}\n",
    "    # Model Building\n",
    "    for model_name, model in models.items():\n",
    "        # train model\n",
    "        model.fit(X_train_selected, train_data[target_col])\n",
    "\n",
    "        # Model Evaluation (e.g., accuracy score)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        model_scores = calculate_metrics(y_test=test_data[target_col], y_pred=y_pred)\n",
    "        models_results_dict[model_name] = model_scores\n",
    "\n",
    "    # Add the selected feature names or indices to the dictionary\n",
    "    models_results_dict[\"selected features\"] = list(train_data.columns[selector.get_support()])\n",
    "    \n",
    "    folds_results[str(fold_num)] = models_results_dict\n",
    "\n",
    "mutual_info_results_df = pd.DataFrame.from_dict(folds_results) \n",
    "mutual_info_results_df.to_csv(\"model_results/mutual_info_feature_selection.csv\", index=False, header=True)\n",
    "\n",
    "mutual_info_results_df = pd.DataFrame(folds_results)\n",
    "mutual_info_results_df.to_excel(\"model_results/mutual_info_feature_selectio.xlsx\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pklimanek\\Desktop\\mgr_2\\HeartFailure_Classification\\main.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pklimanek/Desktop/mgr_2/HeartFailure_Classification/main.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Feature Selection Chi-Squared (χ²): Chi-squared tests can be used for feature selection when dealing with categorical data. \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pklimanek/Desktop/mgr_2/HeartFailure_Classification/main.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# It measures the dependency between variables and is particularly useful for feature selection in classification tasks with categorical features.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pklimanek/Desktop/mgr_2/HeartFailure_Classification/main.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m selector \u001b[39m=\u001b[39m SelectKBest(score_func\u001b[39m=\u001b[39mchi2, k\u001b[39m=\u001b[39mk_best_features)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/pklimanek/Desktop/mgr_2/HeartFailure_Classification/main.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m X_train_selected \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mfit_transform(X_train_scaled, train_data[target_col])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pklimanek/Desktop/mgr_2/HeartFailure_Classification/main.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m X_test_selected \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39mtransform(X_test_scaled)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pklimanek/Desktop/mgr_2/HeartFailure_Classification/main.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m models_results_dict \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    915\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:503\u001b[0m, in \u001b[0;36m_BaseFilter.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    498\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m    499\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m], multi_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    500\u001b[0m )\n\u001b[0;32m    502\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params(X, y)\n\u001b[1;32m--> 503\u001b[0m score_func_ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscore_func(X, y)\n\u001b[0;32m    504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(score_func_ret, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m    505\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscores_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpvalues_ \u001b[39m=\u001b[39m score_func_ret\n",
      "File \u001b[1;32mc:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m global_skip_validation \u001b[39m=\u001b[39m get_config()[\u001b[39m\"\u001b[39m\u001b[39mskip_parameter_validation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    186\u001b[0m func_sig \u001b[39m=\u001b[39m signature(func)\n\u001b[0;32m    188\u001b[0m \u001b[39m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:231\u001b[0m, in \u001b[0;36mchi2\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m    229\u001b[0m X \u001b[39m=\u001b[39m check_array(X, accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, dtype\u001b[39m=\u001b[39m(np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32))\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39many((X\u001b[39m.\u001b[39mdata \u001b[39mif\u001b[39;00m issparse(X) \u001b[39melse\u001b[39;00m X) \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m--> 231\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInput X must be non-negative.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    233\u001b[0m \u001b[39m# Use a sparse representation for Y by default to reduce memory usage when\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[39m# y has many unique classes.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m Y \u001b[39m=\u001b[39m LabelBinarizer(sparse_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mfit_transform(y)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X must be non-negative."
     ]
    }
   ],
   "source": [
    "# folds_results = {}\n",
    "# \n",
    "# for fold_num, (train_data, test_data) in enumerate(zip(train_datasets, test_datasets),1):\n",
    "#     # Standardize features using StandardScaler\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(train_data.drop(target_col, axis=1))\n",
    "#     X_test_scaled = scaler.transform(test_data.drop(target_col, axis=1))\n",
    "\n",
    "#     # Feature Selection Chi-Squared (χ²): Chi-squared tests can be used for feature selection when dealing with categorical data. \n",
    "#     # It measures the dependency between variables and is particularly useful for feature selection in classification tasks with categorical features.\n",
    "#     selector = SelectKBest(score_func=chi2, k=k_best_features)\n",
    "#     X_train_selected = selector.fit_transform(X_train_scaled, train_data[target_col])\n",
    "#     X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "#     models_results_dict = {}\n",
    "#     # Model Building\n",
    "#     for model_name, model in models.items():\n",
    "#         # train model\n",
    "#         model.fit(X_train_selected, train_data[target_col])\n",
    "\n",
    "#         # Model Evaluation (e.g., accuracy score)\n",
    "#         y_pred = model.predict(X_test_selected)\n",
    "#         model_scores = calculate_metrics(y_test=test_data[target_col], y_pred=y_pred)\n",
    "#         models_results_dict[model_name] = model_scores\n",
    "    \n",
    "#     folds_results[fold_num] = models_results_dict\n",
    "\n",
    "# chi2_results_df = pd.DataFrame.from_dict(folds_results) \n",
    "# chi2_results_df.to_csv(\"model_evaluation_results_chi2_feature_selection.csv\", index=False, header=True)\n",
    "\n",
    "# chi2_results_df = pd.DataFrame(folds_results)\n",
    "# chi2_results_df.to_excel(\"model_evaluation_results_chi2_feature_selection.xlsx\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "folds_results = {}\n",
    "\n",
    "for fold_num, (train_data, test_data) in enumerate(zip(train_datasets, test_datasets),1):\n",
    "    # Standardize features using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(train_data.drop(target_col, axis=1))\n",
    "    X_test_scaled = scaler.transform(test_data.drop(target_col, axis=1))\n",
    "\n",
    "    # Feature Selection Recursive Feature Elimination (RFE): RFE is an iterative feature selection method that recursively removes the least significant features. \n",
    "    # It is particularly useful when a prior model (e.g., logistic regression, SVM) in mind.\n",
    "    selector = RFE(estimator=LogisticRegression(), n_features_to_select=k_best_features)\n",
    "    X_train_selected = selector.fit_transform(X_train_scaled, train_data[target_col])\n",
    "    X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "    models_results_dict = {}\n",
    "    # Model Building\n",
    "    for model_name, model in models.items():\n",
    "        # train model\n",
    "        model.fit(X_train_selected, train_data[target_col])\n",
    "\n",
    "        # Model Evaluation (e.g., accuracy score)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        model_scores = calculate_metrics(y_test=test_data[target_col], y_pred=y_pred)\n",
    "        models_results_dict[model_name] = model_scores\n",
    "\n",
    "    # Add the selected feature names or indices to the dictionary\n",
    "    models_results_dict[\"selected features\"] = list(train_data.columns[selector.get_support()])\n",
    "    \n",
    "    folds_results[str(fold_num)] = models_results_dict\n",
    "\n",
    "RFE_results_df = pd.DataFrame.from_dict(folds_results) \n",
    "RFE_results_df.to_csv(\"model_results/RFE_feature_selection.csv\", index=False, header=True)\n",
    "\n",
    "RFE_results_df = pd.DataFrame(folds_results)\n",
    "RFE_results_df.to_excel(\"model_results/RFE_feature_selection.xlsx\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_results = {}\n",
    "\n",
    "for fold_num, (train_data, test_data) in enumerate(zip(train_datasets, test_datasets),1):\n",
    "    # Standardize features using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(train_data.drop(target_col, axis=1))\n",
    "    X_test_scaled = scaler.transform(test_data.drop(target_col, axis=1))\n",
    "\n",
    "    # Feature Selection Recursive Feature Addition (RFA): This is the opposite of RFE. Instead of removing features, it adds features iteratively based on their importance.\n",
    "    selector = RFA(estimator=RandomForestClassifier(), n_features_to_select=k_best_features)\n",
    "    X_train_selected = selector.fit_transform(X_train_scaled, train_data[target_col])\n",
    "    X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "    models_results_dict = {}\n",
    "    # Model Building\n",
    "    for model_name, model in models.items():\n",
    "        # train model\n",
    "        model.fit(X_train_selected, train_data[target_col])\n",
    "\n",
    "        # Model Evaluation (e.g., accuracy score)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        model_scores = calculate_metrics(y_test=test_data[target_col], y_pred=y_pred)\n",
    "        models_results_dict[model_name] = model_scores\n",
    "\n",
    "    # Add the selected feature names or indices to the dictionary\n",
    "    models_results_dict[\"selected features\"] = list(train_data.columns[selector.get_support()])\n",
    "    \n",
    "    folds_results[str(fold_num)] = models_results_dict\n",
    "\n",
    "RFA_results_df = pd.DataFrame.from_dict(folds_results) \n",
    "RFA_results_df.to_csv(\"model_results/RFA_feature_selection.csv\", index=False, header=True)\n",
    "\n",
    "RFA_results_df = pd.DataFrame(folds_results)\n",
    "RFA_results_df.to_excel(\"model_results/RFA_feature_selection.xlsx\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\feature_selection\\_base.py:102: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\feature_selection\\_base.py:102: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(183824, 0)) while a minimum of 1 is required by RandomForestClassifier.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pklimanek\\Desktop\\mgr_2\\HeartFailure_Classification\\main.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pklimanek/Desktop/mgr_2/HeartFailure_Classification/main.ipynb#X32sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Model Building\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pklimanek/Desktop/mgr_2/HeartFailure_Classification/main.ipynb#X32sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m model_name, model \u001b[39min\u001b[39;00m models\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pklimanek/Desktop/mgr_2/HeartFailure_Classification/main.ipynb#X32sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# train model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/pklimanek/Desktop/mgr_2/HeartFailure_Classification/main.ipynb#X32sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X_train_selected, train_data[target_col])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pklimanek/Desktop/mgr_2/HeartFailure_Classification/main.ipynb#X32sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# Model Evaluation (e.g., accuracy score)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pklimanek/Desktop/mgr_2/HeartFailure_Classification/main.ipynb#X32sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test_selected)\n",
      "File \u001b[1;32mc:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:348\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[39mif\u001b[39;00m issparse(y):\n\u001b[0;32m    347\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 348\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    349\u001b[0m     X, y, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mDTYPE\n\u001b[0;32m    350\u001b[0m )\n\u001b[0;32m    351\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32mc:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    619\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    622\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\utils\\validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[1;32m-> 1147\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1148\u001b[0m     X,\n\u001b[0;32m   1149\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1150\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1151\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1152\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1153\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1154\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1155\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1156\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1157\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1158\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1159\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1160\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[0;32m   1163\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\utils\\validation.py:978\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    976\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m    977\u001b[0m     \u001b[39mif\u001b[39;00m n_features \u001b[39m<\u001b[39m ensure_min_features:\n\u001b[1;32m--> 978\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    979\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m feature(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    980\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m a minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    981\u001b[0m             \u001b[39m%\u001b[39m (n_features, array\u001b[39m.\u001b[39mshape, ensure_min_features, context)\n\u001b[0;32m    982\u001b[0m         )\n\u001b[0;32m    984\u001b[0m \u001b[39mif\u001b[39;00m copy:\n\u001b[0;32m    985\u001b[0m     \u001b[39mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m    986\u001b[0m         \u001b[39m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(183824, 0)) while a minimum of 1 is required by RandomForestClassifier."
     ]
    }
   ],
   "source": [
    "# from sklearn.feature_selection import SelectFromModel\n",
    "# from sklearn.linear_model import Lasso\n",
    "\n",
    "# folds_results = {}\n",
    "# for fold_num, (train_data, test_data) in enumerate(zip(train_datasets, test_datasets),1):\n",
    "#     # Standardize features using StandardScaler\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(train_data.drop(target_col, axis=1))\n",
    "#     X_test_scaled = scaler.transform(test_data.drop(target_col, axis=1))\n",
    "\n",
    "#     # Feature Selection L1-based feature selection: L1 regularization methods like Lasso can be used for feature selection. Features with zero coefficients can be pruned.\n",
    "#     selector = SelectFromModel(Lasso(), max_features=k_best_features)\n",
    "#     X_train_selected = selector.fit_transform(X_train_scaled, train_data[target_col])\n",
    "#     X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "#     models_results_dict = {}\n",
    "#     # Model Building\n",
    "#     for model_name, model in models.items():\n",
    "#         # train model\n",
    "#         model.fit(X_train_selected, train_data[target_col])\n",
    "\n",
    "#         # Model Evaluation (e.g., accuracy score)\n",
    "#         y_pred = model.predict(X_test_selected)\n",
    "#         model_scores = calculate_metrics(y_test=test_data[target_col], y_pred=y_pred)\n",
    "#         models_results_dict[model_name] = model_scores\n",
    "\n",
    "#     folds_results[fold_num] = models_results_dict\n",
    "\n",
    "# lasso_results_df = pd.DataFrame.from_dict(folds_results) \n",
    "# lasso_results_df.to_csv(\"model_evaluation_results_lasso_feature_selection.csv\", index=False, header=True)\n",
    "\n",
    "# lasso_results_df = pd.DataFrame(folds_results)\n",
    "# lasso_results_df.to_excel(\"model_evaluation_results_lasso_feature_selection.xlsx\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pklimanek\\Anaconda3\\envs\\magisterka\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "folds_results = {}\n",
    "for fold_num, (train_data, test_data) in enumerate(zip(train_datasets, test_datasets),1):\n",
    "    # Standardize features using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(train_data.drop(target_col, axis=1))\n",
    "    X_test_scaled = scaler.transform(test_data.drop(target_col, axis=1))\n",
    "\n",
    "    # Feature Selection Feature Importance from Tree-based Models: For ensemble tree-based models like Random Forest or XGBoost, you can use feature importances to select the most important features.\n",
    "    selector = SelectFromModel(RandomForestClassifier(), max_features=k_best_features)\n",
    "    X_train_selected = selector.fit_transform(X_train_scaled, train_data[target_col])\n",
    "    X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "    models_results_dict = {}\n",
    "    # Model Building\n",
    "    for model_name, model in models.items():\n",
    "        # train model\n",
    "        model.fit(X_train_selected, train_data[target_col])\n",
    "\n",
    "        # Model Evaluation (e.g., accuracy score)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        model_scores = calculate_metrics(y_test=test_data[target_col], y_pred=y_pred)\n",
    "        models_results_dict[model_name] = model_scores\n",
    "\n",
    "    # Add the selected feature names or indices to the dictionary\n",
    "    models_results_dict[\"selected features\"] = list(train_data.columns[selector.get_support()])\n",
    "    \n",
    "    folds_results[str(fold_num)] = models_results_dict\n",
    "\n",
    "tree_feature_importance_results_df = pd.DataFrame.from_dict(folds_results) \n",
    "tree_feature_importance_results_df.to_csv(\"model_results/tree_feature_importance_feature_selection.csv\", index=False, header=True)\n",
    "\n",
    "tree_feature_importance_results_df = pd.DataFrame(folds_results)\n",
    "tree_feature_importance_results_df.to_excel(\"model_results/tree_feature_importance_feature_selection.xlsx\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magisterka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
