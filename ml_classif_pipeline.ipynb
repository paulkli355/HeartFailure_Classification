{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import chi2, mutual_info_classif, VarianceThreshold, SelectFromModel\n",
    "from skfeature.function.similarity_based import fisher_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
    "\n",
    "from summarytools import dfSummary\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.pipeline import Pipeline as Pipeline_imb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook parameters\n",
    "\n",
    "input_dataset_path = \"data/heart_disease_health_indicators_BRFSS2015.csv\"\n",
    "target_col = \"HeartDiseaseorAttack\"\n",
    "separator = ','\n",
    "generate_new_folds = False\n",
    "n_splits = 5\n",
    "k_best_features = 5\n",
    "\n",
    "fix_imbalanced_dataset = True\n",
    "use_oversampling = False\n",
    "use_undersampling = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df = pd.read_csv(input_dataset_path, sep=separator)\n",
    "heart_df.drop(columns=[col for col in heart_df.columns if col.lower()=='id'], inplace=True)\n",
    "if all(isinstance(item, (int, float, np.int32, np.int64)) for item in list(heart_df[target_col].unique())):\n",
    "    heart_df[target_col] = heart_df[target_col].astype(int)\n",
    "heart_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Check if there are NaN values present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert heart_df[heart_df.isna().any(axis=1)].empty, 'Dataset contains NaN values!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Get infromation about dataset shape and target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general dataset descriptors\n",
    "print(f\"Input dataset has {heart_df.shape[0]} rows and {heart_df.shape[1]} colums\")\n",
    "print(f\"Input dataset consists of {heart_df.drop(columns=[target_col]).shape[1]} features and 1 target column\")\n",
    "\n",
    "print(f\"Target values are: {heart_df[target_col].unique()}\")\n",
    "print(f\"Number of classes in target: {heart_df[target_col].unique().shape[0]}\")\n",
    "print(f\"Input dataset contains {heart_df[heart_df.duplicated()].shape[0]} duplicated rows and {heart_df[heart_df.duplicated()==False].shape[0]} unique rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Check if the target is balanced - the target value distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(x=heart_df[target_col].value_counts().values,labels=heart_df[target_col].unique().tolist(),autopct='%1.2f%%')\n",
    "plt.title('Target Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Print features correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = heart_df.corr()\n",
    "f,ax=plt.subplots(figsize=(10,7))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", linewidths=0.5, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSummary(heart_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot on each feature\n",
    "plt.figure(figsize=(20,60))\n",
    "for i,column in enumerate(heart_df.columns):\n",
    "    plt.subplot(len(heart_df.columns), 5, i+1)\n",
    "    plt.suptitle(\"Plot Value Count\", fontsize=20, x=0.5, y=1)\n",
    "    sns.countplot(data=heart_df, x=column)\n",
    "    plt.title(f\"{column}\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_features = ['HighBP', 'HighChol', 'CholCheck','Smoker', 'Stroke','PhysActivity', 'Fruits', 'Veggies', \n",
    "                'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'DiffWalk', 'Sex']\n",
    "cat_features = ['Diabetes', 'GenHlth', 'Education','Income']\n",
    "num_features = ['BMI','MentHlth', 'PhysHlth', 'Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(heart_df)\n",
    "print(heart_df.shape)\n",
    "l=list(np.arange(22))\n",
    "print(l)\n",
    "plt.title(\"data distribution\")\n",
    "plt.xticks(l, list(heart_df.columns),\n",
    "       rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Drop duplicated values and separate features from target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all duplicated values within the dataset\n",
    "heart_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Encode columns tahat have categorical data with LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {}\n",
    "\n",
    "# Iterate through columns with categorical data\n",
    "for column in heart_df.columns:\n",
    "    if heart_df[column].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        heart_df[column] = le.fit_transform(heart_df[column])\n",
    "        label_encoders[column] = le\n",
    "        print(f'{column} was encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Decode the encoded data\n",
    "# decoded_heart_df = pd.DataFrame()\n",
    "# for column in heart_df.columns:\n",
    "#     if column in label_encoders:\n",
    "#         decoded_values = label_encoders[column].inverse_transform(heart_df[column])\n",
    "#         decoded_heart_df[column] = decoded_values\n",
    "#     else:\n",
    "#         decoded_heart_df[column] = heart_df[column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Separate features from target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide a heart failure dataset into features and target value sets\n",
    "X = heart_df.drop(columns=[target_col])\n",
    "y = heart_df[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Divide dataset into startified training and testing set (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype(np.float32)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=1,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Detect important features for feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Removal of all feature with low variance (threshold = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vthresh = VarianceThreshold(threshold=0.005)\n",
    "vthresh.fit_transform(X_train)\n",
    "selected_features_vth = vthresh.get_feature_names_out()\n",
    "print(f'Variance Threshold (0.005) selected features are: {selected_features_vth}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Chi2 test to define important features with alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "f_score, p_values = chi2(X_train, y_train)\n",
    "p_values = pd.Series(p_values)\n",
    "p_values.index = X_train.columns\n",
    "p_values.sort_values(ascending=False, inplace=True)\n",
    "selected_features_chi2 = list(p_values[p_values < alpha].index)\n",
    "print(f'Chi2 test selected features are: {selected_features_chi2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Information gain - estimate mutual information with threshold 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = mutual_info_classif(X_train,y_train)\n",
    "feat_importances = pd.Series(importances, X_train.columns).sort_values()\n",
    "feat_importances.plot(kind='barh', color='teal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.01\n",
    "selected_features_mi = list(feat_importances[feat_importances > threshold].index)\n",
    "print(f'Mutual infromation estimation selected features are: {selected_features_mi}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Correlation based feature selection with threshold 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "\n",
    "high_corr_features = set(correlation(X_train, 0.8))\n",
    "selected_features_corr = [item for item in X_train.columns if item not in high_corr_features]\n",
    "print(f'Pearson correlation selected features are: {selected_features_corr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Mean Absolute Difference (MAD) feature selection - works better for continuous / numerical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_abs_diff = np.sum(np.abs(X_train - np.mean(X_train, axis=0)), axis=0) / X_train.shape[0]\n",
    "# mad_feat_importances = pd.Series(mean_abs_diff, X_train.columns).sort_values()\n",
    "# mad_feat_importances.plot(kind='barh', color='teal')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = 0.5\n",
    "# selected_features_mad = list(mad_feat_importances[mad_feat_importances > threshold].index)\n",
    "# print(f'Mean Absolute Difference selected features are: {selected_features_mad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6 LASSO Regularization (L1) feature selection embedded method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X_train, y_train)\n",
    "sfm = SelectFromModel(lsvc, prefit=True)\n",
    "\n",
    "X_train_new = X_train.loc[:, sfm.get_support()]\n",
    "selected_features_l1 = X_train_new.columns\n",
    "print(f'Lasso L1 Regularization selected features are: {selected_features_l1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7 Important features that were selected across all methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vrh_sf = set(selected_features_vth)\n",
    "chi2_sf = set(selected_features_chi2)\n",
    "mi_sf = set(selected_features_mi)\n",
    "corr_sf = set(selected_features_corr)\n",
    "l1_sf = set(selected_features_l1)\n",
    "\n",
    "common_features = list(vrh_sf.intersection(chi2_sf, mi_sf, corr_sf, l1_sf))\n",
    "print(f'Across all methods there were {len(common_features)} selected. These are: {common_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Initialize base classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(multi_class='multinomial',\n",
    "                          solver='newton-cg',\n",
    "                          random_state=1)\n",
    "clf2 = KNeighborsClassifier(algorithm='ball_tree',\n",
    "                            leaf_size=50)\n",
    "clf3 = DecisionTreeClassifier(random_state=1)\n",
    "clf4 = SVC(kernel=\"linear\", C=0.3, random_state=1)\n",
    "clf5 = RandomForestClassifier(random_state=1)\n",
    "clf6 = XGBClassifier(objective= 'binary:logistic',\n",
    "                    nthread=4,\n",
    "                    seed=42)\n",
    "clf7 = GaussianNB()\n",
    "clf8 = NearestCentroid(metric='euclidean', shrink_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fix_imbalanced_dataset and use_oversampling:\n",
    "    sampler = SMOTE(random_state=42)\n",
    "elif fix_imbalanced_dataset and use_undersampling:\n",
    "    sampler = RandomUnderSampler(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Build pipelines with respected models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fix_imbalanced_dataset:\n",
    "    pipe1 = Pipeline_imb([('sampler', sampler),\n",
    "                          ('scaler', StandardScaler()),\n",
    "                          ('LR', clf1)])\n",
    "\n",
    "    pipe2 = Pipeline_imb([('sampler', sampler),\n",
    "                          ('scaler', StandardScaler()),\n",
    "                          ('KNN', clf2)])\n",
    "    \n",
    "    pipe3 = Pipeline_imb([('sampler', sampler),\n",
    "                          ('DT', clf3)])\n",
    "\n",
    "    pipe4 = Pipeline_imb([('sampler', sampler),\n",
    "                          ('scaler', StandardScaler()),\n",
    "                          ('SVM', clf4)])\n",
    "    \n",
    "    pipe5 = Pipeline_imb([('sampler', sampler),\n",
    "                          ('RF', clf5)])\n",
    "\n",
    "    pipe6 = Pipeline_imb([('sampler', sampler),\n",
    "                          ('XGB', clf6)])\n",
    "\n",
    "    pipe7 = Pipeline_imb([('sampler', sampler),\n",
    "                          ('scaler', StandardScaler()),\n",
    "                          ('GNB', clf7)])\n",
    "\n",
    "    pipe8 = Pipeline_imb([('sampler', sampler),\n",
    "                          ('scaler', StandardScaler()),\n",
    "                          ('NC', clf8)])\n",
    "\n",
    "else:\n",
    "    # Building the pipelines based on pre defined classifiers\n",
    "    pipe1 = Pipeline([('scaler', StandardScaler()),\n",
    "                    ('LR', clf1)])\n",
    "\n",
    "    pipe2 = Pipeline([('scaler', StandardScaler()),\n",
    "                    ('KNN', clf2)])\n",
    "\n",
    "    pipe4 = Pipeline([('scaler', StandardScaler()),\n",
    "                    ('SVM', clf4)])\n",
    "\n",
    "    pipe7 = Pipeline([('scaler', StandardScaler()),\n",
    "                    ('GNB', clf7)])\n",
    "\n",
    "    pipe8 = Pipeline([('scaler', StandardScaler()),\n",
    "                    ('NC', clf8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Set up parameter grids for GridSearchCV hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression parameters\n",
    "param_grid1 = [{'LR__penalty': ['l1', 'l2'],\n",
    "                'LR__C': np.power(10., np.arange(-4, 4))}]\n",
    "\n",
    "# KNN parameters\n",
    "param_grid2 = [{'KNN__n_neighbors': list(range(1, 10)),\n",
    "                'KNN__p': [1, 2]}]\n",
    "\n",
    "# Decision Trees parameters\n",
    "param_grid3 = [{'DT__max_depth': list(range(1, 10)) + [None],\n",
    "                'DT__criterion': ['gini', 'entropy']}]\n",
    "\n",
    "# SVM parameters\n",
    "param_grid4 = [{'SVM__kernel': ['rbf'],\n",
    "                'SVM__C': np.power(10., np.arange(-4, 4)),\n",
    "                'SVM__gamma': np.power(10., np.arange(-5, 0))},\n",
    "               {'SVM__kernel': ['linear'],\n",
    "                'SVM__C': np.power(10., np.arange(-4, 4))}]\n",
    "\n",
    "# Random Forest parameters\n",
    "param_grid5 = [{'RF__n_estimators': [10, 100, 500, 1000, 10000]}]\n",
    "\n",
    "# XGBoost parameters\n",
    "param_grid6 = [{'XGB__max_depth': range (2, 10, 1),\n",
    "               'XGB__n_estimators': range(60, 220, 40),\n",
    "               'XGB__learning_rate': [0.1, 0.01, 0.05]}]\n",
    "\n",
    "# Gausian Naive Bayes parameters\n",
    "param_grid7 = [{'GNB__var_smoothing': np.logspace(0,-9, num=100)}]\n",
    "\n",
    "# Nearest Centraid parameters\n",
    "param_grid8 = [{'NC__shrink_threshold': np.arange(0, 1.01, 0.01),\n",
    "                'NC__metric': ['euclidean', 'manhattan']}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring = {\n",
    "#     'accuracy' : make_scorer(accuracy_score), \n",
    "#     'precision' : make_scorer(precision_score),\n",
    "#     'recall' : make_scorer(recall_score), \n",
    "#     'f1_score' : make_scorer(f1_score),\n",
    "#     'mcc': make_scorer(matthews_corrcoef),\n",
    "#     'roc_auc_score': make_scorer(roc_auc_score)\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4. Set up multiple GridSearchCV objects, 1 for each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridcvs = {}\n",
    "inner_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=1)\n",
    "\n",
    "for pgrid, est, name in zip((param_grid3, param_grid7, param_grid8, param_grid5, param_grid1, param_grid6),\n",
    "                            (pipe3, pipe7, pipe8, pipe5, pipe1, pipe6),\n",
    "                            ('DT', 'GNB', 'NC', 'RF', 'LR', 'XGB')):\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                    #    scoring='accuracy',\n",
    "                    #    scoring=scoring,\n",
    "                       scoring='f1',\n",
    "                       n_jobs=-1,\n",
    "                       cv=inner_cv,\n",
    "                       verbose=0,\n",
    "                       refit=True)\n",
    "    gridcvs[name] = gcv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 Tarin models and find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, gs_est in sorted(gridcvs.items()):\n",
    "\n",
    "    print(50 * '-', '\\n')\n",
    "    print('Classification algorithm:', name)\n",
    "    print('    Inner loop:')\n",
    "    \n",
    "    outer_scores = []\n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "    for train_idx, valid_idx in outer_cv.split(X_train, y_train):\n",
    "\n",
    "        gridcvs[name].fit(X_train[common_features].iloc[train_idx], y_train.iloc[train_idx]) # run inner loop hyperparam tuning\n",
    "        print('\\n        Best F1 score (avg. of inner test folds) %.4f' % (gridcvs[name].best_score_))\n",
    "        print('        Best parameters:', gridcvs[name].best_params_)\n",
    "        \n",
    "        # perf on test fold (valid_idx)\n",
    "        outer_scores.append(gridcvs[name].best_estimator_.score(X_train[common_features].iloc[valid_idx], y_train.iloc[valid_idx]))\n",
    "        print('        F1 score (on outer test fold) %.4f' % (outer_scores[-1]))\n",
    "    \n",
    "    print('\\n    Outer Loop:')\n",
    "    print('        F1 score %.4f +/- %.4f' % (np.mean(outer_scores), np.std(outer_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Best model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv_model_select = GridSearchCV(estimator=pipe6,\n",
    "                                param_grid=param_grid6,\n",
    "                                scoring='f1',\n",
    "                                n_jobs=-1,\n",
    "                                cv=outer_cv,\n",
    "                                verbose=1,\n",
    "                                refit=True)\n",
    "\n",
    "gcv_model_select.fit(X_train[common_features], y_train)\n",
    "print('Best CV F1 score: %.4f' % (gcv_model_select.best_score_))\n",
    "print('Best parameters:', gcv_model_select.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Best model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can skip the next step because we set refit=True\n",
    "## so scikit-learn has already fit the model to the\n",
    "## whole training set\n",
    "\n",
    "# gcv_model_select.fit(X_train, y_train)\n",
    "\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=gcv_model_select.predict(X_train[common_features]))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=gcv_model_select.predict(X_test[common_features]))\n",
    "\n",
    "train_recall = recall_score(y_true=y_train, y_pred=gcv_model_select.predict(X_train[common_features]))\n",
    "test_recall = recall_score(y_true=y_test, y_pred=gcv_model_select.predict(X_test[common_features]))\n",
    "\n",
    "train_precision = precision_score(y_true=y_train, y_pred=gcv_model_select.predict(X_train[common_features]))\n",
    "test_precision = precision_score(y_true=y_test, y_pred=gcv_model_select.predict(X_test[common_features]))\n",
    "\n",
    "train_f1 = f1_score(y_true=y_train, y_pred=gcv_model_select.predict(X_train[common_features]))\n",
    "test_f1 = f1_score(y_true=y_test, y_pred=gcv_model_select.predict(X_test[common_features]))\n",
    "\n",
    "tarin_mcc = matthews_corrcoef(y_true=y_train, y_pred=gcv_model_select.predict(X_train[common_features]))\n",
    "test_mcc = matthews_corrcoef(y_true=y_test, y_pred=gcv_model_select.predict(X_test[common_features]))\n",
    "\n",
    "tarin_roc_auc = roc_auc_score(y_true=y_train, y_score=gcv_model_select.predict(X_train[common_features]))\n",
    "test_roc_auc = roc_auc_score(y_true=y_test, y_score=gcv_model_select.predict(X_test[common_features]))\n",
    "\n",
    "train_scores = {\n",
    "    'ACC': train_acc,\n",
    "    'Recall': train_recall,\n",
    "    'Precision': train_precision,\n",
    "    'F1 score': train_f1,\n",
    "    'MCC': tarin_mcc,\n",
    "    'ROC AUC score': tarin_roc_auc\n",
    "}\n",
    "\n",
    "train_scores_df = pd.DataFrame(list(train_scores.items()), columns=['Metric name', 'Metric value'])\n",
    "train_scores_df['Dataset'] = 'training'\n",
    "\n",
    "test_scores = {\n",
    "    'ACC': test_acc,\n",
    "    'Recall': test_recall,\n",
    "    'Precision': test_precision,\n",
    "    'F1 score': test_f1,\n",
    "    'MCC': test_mcc,\n",
    "    'ROC AUC score': test_roc_auc\n",
    "}\n",
    "\n",
    "test_scores_df = pd.DataFrame(list(test_scores.items()), columns=['Metric name', 'Metric value'])\n",
    "test_scores_df['Dataset'] = 'testing'\n",
    "\n",
    "model_scores_df = pd.concat([train_scores_df, test_scores_df], axis=0)\n",
    "\n",
    "# print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "# print('Test Accuracy: %.2f%%' % (100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Save trained best model into pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "with open(f'models/xgb_best_model_{timestamp}_randomUndersampling.pickle', 'wb') as file:\n",
    "    pickle.dump(gcv_model_select, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "\n",
    "model_scores_df.to_excel(f'models/xgb_best_model_results_{timestamp}_randomUndersampling.xlsx', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magisterka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
